{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.datasets import WMT14\n",
    "from torchtext.datasets import IWSLT \n",
    "from torchtext.data import Field, BucketIterator,Iterator\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/soumya/anaconda3/lib/python3.7/site-packages/spacy/util.py:271: UserWarning: [W031] Model 'en_core_web_sm' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.0). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "spacy_en = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de_core_news_sm-2.3.0/de_core_news_sm/de_core_news_sm-2.3.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_fr = spacy.load('fr_core_news_sm-2.3.0/fr_core_news_sm/fr_core_news_sm-2.3.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_fr(text):\n",
    "    return [tok.text for tok in spacy_fr.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_en,init_token='<sos>',eos_token=\n",
    "           '<eos>',lower=True,batch_first=True,include_lengths=True)\n",
    "TRG = Field(tokenize=tokenize_de,init_token='<sos>',eos_token=\n",
    "           '<eos>',lower=True,batch_first=True,include_lengths=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data , valid_data , test_data = Multi30k.splits(exts=('.de','.en'),fields = (SRC,TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t_data , v_data , te_data = IWSLT.splits(exts=('.de','.en'),fields = (SRC,TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data,min_freq = 2)\n",
    "TRG.build_vocab(train_data,min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BATCH_SIZE = 128\n",
    "\n",
    "#train_iterator , valid_iterator , test_iterator = BucketIterator.splits(\n",
    "#    (train_data,valid_data,test_data),batch_size=BATCH_SIZE,device=device,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_iterator , valid_iterator , test_iterator = Iterator.splits(\n",
    "    (train_data,valid_data,test_data),batch_size=BATCH_SIZE,device=device,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim,emb_dim,enc_hid_dim,dec_hid_dim,drop):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim,emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim,enc_hid_dim,bidirectional=True,batch_first=True)\n",
    "        self.fc = nn.Linear(enc_hid_dim*2,dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(drop)\n",
    "        self.activ = nn.Tanh()\n",
    "    def forward(self,src,src_len):\n",
    "        #print(\"Input Shape\",src.shape)\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        packed_src = nn.utils.rnn.pack_padded_sequence(embedded,src_len,\n",
    "                                                       batch_first=True,\n",
    "                                                       enforce_sorted=False)\n",
    "        packed_outputs , hidden = self.rnn(packed_src)\n",
    "        outputs,_ = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
    "        #print(\"Encoder Shape in Encoder Output \",outputs.shape)\n",
    "        outputs = outputs.permute(1,0,2)\n",
    "        # outputs ---> batch x seq x enc_hid_dim\n",
    "        # hidden ----> 2 x batch x enc_hid_dim\n",
    "        # hidden[-2,:,:] batch * enc_hid_dim\n",
    "        #hidden = self.activ(self.fc(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1)))\n",
    "        # Adding Dropout version\n",
    "        hidden = self.dropout(self.activ(self.fc(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1))))\n",
    "        return outputs,hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an implementation of Bahdanau (ICLR 2014 ) Attention architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,enc_hid_dim,dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_hid_dim*2+dec_hid_dim,dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim,1,bias=False)\n",
    "        self.actv = nn.Tanh()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self,hidden,encoder_outputs,mask):\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        #print(\"Hidden Shape Pre \",hidden.shape)\n",
    "        #print(\"Encoder Shape in Attention Layer\",encoder_outputs.shape)\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        #print(\"Hidden Shape Post \",hidden.shape)\n",
    "        concat_input = torch.cat((hidden, encoder_outputs), dim = 2)\n",
    "        #print(\"Concatenated in Attention Layer\",concat_input.shape)\n",
    "        #print(\"Linear Layer Shape \",self.attn.weight.shape)\n",
    "        # torch.cat((hidden, encoder_outputs) -->  batch x seq x (enc_hid_dim*2)+dec_hid_dim\n",
    "        #energy = self.actv(self.attn(concat_input))\n",
    "        # Dropout Added \n",
    "        energy = self.dropout(self.actv(self.attn(concat_input)))\n",
    "        # attention --->  batch * seq * 1 \n",
    "        #print(\"Energy \",energy.shape)\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        #print(\"Attention \",attention.shape)\n",
    "        # attention --> batch x seq \n",
    "        attention = attention.masked_fill(mask==1,1e-10)\n",
    "        return F.softmax(attention, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,out_dim,attn,dec_hid_dim,emb_dim,enc_hid_dim,drop):\n",
    "        super().__init__()\n",
    "        self.attn = attn\n",
    "        self.out_dim = out_dim\n",
    "        self.embed = nn.Embedding(out_dim,emb_dim)\n",
    "        self.fc = nn.Linear(dec_hid_dim+enc_hid_dim*2+emb_dim,out_dim)\n",
    "        self.rnn = nn.GRU(emb_dim+enc_hid_dim*2,dec_hid_dim,batch_first=True,bidirectional=False)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "    def forward(self,inputs,hidden,encoder_states,mask):\n",
    "        # inputs --->  batch * 1\n",
    "        #print(\"Decoding Input Shape\",inputs.shape)\n",
    "        embedded = self.drop(self.embed(inputs))\n",
    "        # embedded --> batch * 1 * emb_dim \n",
    "        att_weights = self.attn(hidden,encoder_states,mask)\n",
    "        att_weights = att_weights.unsqueeze(2)\n",
    "        #print(\"Att weights \",att_weights.shape)\n",
    "        # att_weights --> batch x seq x 1\n",
    "        # encoder_states --> batch x seq x 2*enc_hid_dim\n",
    "        #print(\"Pre Encoder States \",encoder_states.shape)\n",
    "        encoder_states = encoder_states.permute(0,2,1)\n",
    "        #print(\"Post Encoder States \",encoder_states.shape)\n",
    "        context_vector = torch.bmm(encoder_states,att_weights)\n",
    "        #print(\"Pre context vector \",context_vector.shape)\n",
    "        #context_vector -->  batch x 2*enc_hid_dim * 1\n",
    "        context_vector = context_vector.permute(0,2,1)\n",
    "        #print(\"Post context vector \",context_vector.shape)\n",
    "        #print(\"Pre Embedded vector \",embedded.shape)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        #print(\"Post Embedded vector \",embedded.shape)\n",
    "        #context_vector -->  batch x  1 x 2* enc_hid_dim\n",
    "        concat_input = torch.cat((context_vector,embedded),dim=2)\n",
    "        #print(\"Concat Input Shape in Decoder \",concat_input.shape)\n",
    "        hidden = hidden.unsqueeze(0)\n",
    "        out , hidden  = self.rnn(concat_input,hidden)\n",
    "        #out -->  batch * 1 * 2*dec_hid_dim\n",
    "        out = out.squeeze(1)\n",
    "        #out -->  batch x 2*dec_hid_dim\n",
    "        embedded = embedded.squeeze(1)\n",
    "        # embedded --> batch x emb_dim\n",
    "        context_vector = context_vector.squeeze(1)\n",
    "        #context_vector -->  batch x 2* enc_hid_dim\n",
    "        prediction = self.fc(torch.cat((out,embedded,context_vector),dim=1))\n",
    "        return prediction,att_weights,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,attn,out_dim,input_dim,dec_hid_dim,\n",
    "                 enc_hid_dim,enc_emb_dim,dec_emb_dim,drop,pad_tok,device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.attn  = attn\n",
    "        self.decoder = decoder\n",
    "        self.out_dim = out_dim\n",
    "        self.enc_hid_dim = enc_hid_dim\n",
    "        self.dec_hid_dim = dec_hid_dim\n",
    "        self.drop = drop\n",
    "        self.pad_tok = pad_tok\n",
    "        self.device = device\n",
    "        #self.teacher_force_ratio = teacher_force_ratio\n",
    "    \n",
    "    def create_mask(self,src):\n",
    "        mask = (src == self.pad_tok).to(torch.int8)\n",
    "        return mask\n",
    "           \n",
    "    def forward(self,src,src_len,trg,teacher_force_ratio=True):\n",
    "        batch_sz = src.size(0)\n",
    "        src_seqlen = src.size(1)\n",
    "        mask = self.create_mask(src)\n",
    "        outputs , hidden = self.encoder(src,src_len)\n",
    "        trg_len = trg.size(1)\n",
    "        attention_weights = torch.zeros((trg_len,batch_sz,src_seqlen)).to(self.device)\n",
    "        # For storing attention per target token\n",
    "        total_outputs = torch.zeros((trg_len,batch_sz,self.out_dim)).to(self.device)\n",
    "        inputs =  trg[:,0]\n",
    "        #print(\"Initial Input Shape\",inputs.shape)\n",
    "        for idx in range(1,trg_len,1):\n",
    "            #print(\"Decoding token \",idx)\n",
    "            prediction, attention_wt , hidden = self.decoder(\n",
    "                inputs,hidden,outputs,mask.to(self.device))\n",
    "            #print(\"Prediction Shape\",prediction.shape)\n",
    "            #print(\"attention weight Shape\",attention_wt.shape)\n",
    "            #print(\"Hidden Shape\",hidden.shape)\n",
    "            attention_weights[idx,:,:] = attention_wt.squeeze(2)\n",
    "            total_outputs[idx,:,:] = prediction\n",
    "            hidden = hidden.squeeze(0)\n",
    "            inputs = prediction.argmax(dim=1)\n",
    "            #total_outputs[idx,:,:] = inputs\n",
    "            #print(\"Input Shape while decoding\",inputs.shape)\n",
    "            if teacher_force_ratio:\n",
    "                flag = torch.rand(1).item() >= 0.65 \n",
    "                if flag:\n",
    "                    inputs =  prediction.argmax(dim=1)\n",
    "                else:\n",
    "                    inputs = trg[:,idx]\n",
    "            #inputs =  prediction.argmax(dim=1)\n",
    "        return total_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_param_init(model):\n",
    "    for name,param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            nn.init.normal_(param.data,mean=0,std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data,0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "ENC_HID_DIM = 512\n",
    "DEC_HID_DIM = 512\n",
    "DROPOUT = 0.5\n",
    "TEACHER_FORCE = 1\n",
    "SRC_PAD_IDX = SRC.vocab.stoi[SRC.pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7873, 256)\n",
       "    (rnn): GRU(256, 512, batch_first=True, bidirectional=True)\n",
       "    (fc): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (activ): Tanh()\n",
       "  )\n",
       "  (attn): Attention(\n",
       "    (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "    (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    (actv): Tanh()\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attn): Attention(\n",
       "      (attn): Linear(in_features=1536, out_features=512, bias=True)\n",
       "      (v): Linear(in_features=512, out_features=1, bias=False)\n",
       "      (actv): Tanh()\n",
       "      (dropout): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (embed): Embedding(5972, 256)\n",
       "    (fc): Linear(in_features=1792, out_features=5972, bias=True)\n",
       "    (rnn): GRU(1280, 512, batch_first=True)\n",
       "    (drop): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(INPUT_DIM,ENC_EMB_DIM,ENC_HID_DIM,DEC_HID_DIM,DROPOUT)\n",
    "attn  = Attention(ENC_HID_DIM,DEC_HID_DIM)\n",
    "decoder = Decoder(OUTPUT_DIM,attn,DEC_HID_DIM,DEC_EMB_DIM,\n",
    "                  ENC_HID_DIM,DROPOUT)\n",
    "model = Seq2Seq(encoder,decoder,attn,OUTPUT_DIM,INPUT_DIM,DEC_HID_DIM,ENC_HID_DIM,ENC_EMB_DIM,\n",
    "                DEC_EMB_DIM,DROPOUT,SRC_PAD_IDX,device)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 20,685,396 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,iterator,optimizer,criterion,clip):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    for i,batch in enumerate(iterator):\n",
    "        src , src_len = batch.src\n",
    "        trg, trg_len = batch.trg\n",
    "        #print(src)\n",
    "        #print(src_len)\n",
    "        #print(trg)\n",
    "        #print(src,src_len)\n",
    "        #print(trg,trg_len)\n",
    "        outputs = model(src,src_len,trg)\n",
    "        #outputs = model(src,src_len,trg)\n",
    "        outputs = outputs[1:].view(-1,OUTPUT_DIM)\n",
    "        #print(\"Pred output shape \",outputs.shape)\n",
    "        trg = trg.permute(1,0)\n",
    "        trg = trg[1:]\n",
    "        trg = trg.reshape(-1)\n",
    "        #print(\"Gold output shape \",trg.shape)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs,trg)\n",
    "        epoch_loss = epoch_loss + loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(),clip)\n",
    "        optimizer.step()\n",
    "    \n",
    "    return (epoch_loss)/len(iterator)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,iterator,criterion):\n",
    "    epoch_loss = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(iterator):\n",
    "            src , src_len = batch.src\n",
    "            trg, trg_len = batch.trg\n",
    "            #print(\"SRC Shape \",src.shape)\n",
    "            outputs = model(src,src_len,trg,False)\n",
    "            outputs = outputs[1:].reshape(-1,OUTPUT_DIM)\n",
    "            #print(\"Outputs Shape \",outputs.shape)\n",
    "            trg = trg.permute(1,0)\n",
    "            trg = trg[1:]\n",
    "            trg = trg.reshape(-1)\n",
    "            #print(\"target Shape \",trg.shape)\n",
    "            #print(\"Outputs Shape \",outputs.shape)\n",
    "            loss = criterion(outputs,trg)\n",
    "            epoch_loss = epoch_loss + loss.item()\n",
    "\n",
    "    return (epoch_loss)/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch-----> 0\n",
      "\tTrain Loss: 3.997 | Train PPL:  54.423\n",
      "\t Val. Loss: 3.602 |  Val. PPL:  36.678\n",
      " Epoch-----> 1\n",
      "\tTrain Loss: 2.835 | Train PPL:  17.031\n",
      "\t Val. Loss: 3.372 |  Val. PPL:  29.129\n",
      " Epoch-----> 2\n",
      "\tTrain Loss: 2.392 | Train PPL:  10.940\n",
      "\t Val. Loss: 3.313 |  Val. PPL:  27.455\n",
      " Epoch-----> 3\n",
      "\tTrain Loss: 2.135 | Train PPL:   8.457\n",
      "\t Val. Loss: 3.305 |  Val. PPL:  27.247\n",
      " Epoch-----> 4\n",
      "\tTrain Loss: 1.910 | Train PPL:   6.754\n",
      "\t Val. Loss: 3.352 |  Val. PPL:  28.573\n",
      " Epoch-----> 5\n",
      "\tTrain Loss: 1.774 | Train PPL:   5.893\n",
      "\t Val. Loss: 3.421 |  Val. PPL:  30.613\n",
      " Epoch-----> 6\n",
      "\tTrain Loss: 1.651 | Train PPL:   5.214\n",
      "\t Val. Loss: 3.531 |  Val. PPL:  34.156\n",
      " Epoch-----> 7\n",
      "\tTrain Loss: 1.567 | Train PPL:   4.791\n",
      "\t Val. Loss: 3.529 |  Val. PPL:  34.074\n",
      " Epoch-----> 8\n",
      "\tTrain Loss: 1.506 | Train PPL:   4.509\n",
      "\t Val. Loss: 3.555 |  Val. PPL:  34.997\n",
      " Epoch-----> 9\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "clip = 1\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f' Epoch-----> {epoch}')\n",
    "    train_loss = train(model,train_iterator,optimizer,criterion,clip)\n",
    "    valid_loss = evaluate(model,valid_iterator,criterion)\n",
    "    if valid_loss <= best_loss:\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'translation_model.pt')\n",
    "        \n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t| Test Loss: 3.3549 |  Test PPL: 28.643 |\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('translation_model.pt'))\n",
    "\n",
    "test_loss = evaluate(model,test_iterator,criterion)\n",
    "\n",
    "print(f'\\t| Test Loss: {test_loss:.4f} |  Test PPL: {math.exp(test_loss):4.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence,src_field,trg_field,model,max_len=50):\n",
    "    model.eval()\n",
    "    tokens = [tok.lower() for tok in tokenize_de(sentence)]\n",
    "    print(f'Post tokenization: {tokens}')\n",
    "    tokens = [src_field.init_token] + tokens + [src_field.eos_token]\n",
    "    numeralized_token = [SRC.vocab.stoi[tok] for tok in tokens]\n",
    "    inputs = torch.LongTensor(numeralized_token).reshape(1,len(numeralized_token))\n",
    "    input_len = torch.LongTensor([len(numeralized_token)])    \n",
    "    out , hidden = encoder(inputs.to(device),input_len.to(device))\n",
    "    mask = torch.LongTensor([0 for _ in range(out.size(1))]).resize(1,len(numeralized_token))\n",
    "    inputs = torch.LongTensor([src_field.vocab.stoi[src_field.init_token]])\n",
    "    \n",
    "    attentions = []\n",
    "    predictions = []\n",
    "    for i in range(50):\n",
    "        prediction, attention_wt , hidden = decoder(inputs.to(device),\n",
    "                                                    hidden.to(device),\n",
    "                                                    out.to(device),\n",
    "                                                    mask.to(device))     \n",
    "        attentions.append(attention_wt.squeeze(2))\n",
    "        hidden = hidden.squeeze(0)\n",
    "        inputs = prediction.argmax(dim=1)\n",
    "        predictions.append(inputs)\n",
    "        if inputs.item() == trg_field.vocab.stoi[trg_field.eos_token]:\n",
    "            break\n",
    "    return predictions,attentions\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent =  \"einer Stadt .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post tokenization: ['einer', 'stadt', '.']\n"
     ]
    }
   ],
   "source": [
    "prediction,attentions = translate(example_sent,SRC,TRG,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "translated_sent = [TRG.vocab.itos[tok.item()] for tok in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a city city . <eos>'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(translated_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
